{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c6fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dc222cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericNetwork(nn.Module):\n",
    "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions=1, is_actor=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_actor = is_actor\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
    "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\n",
    "\n",
    "        if self.is_actor:\n",
    "            # Actor outputs mean and log std\n",
    "            self.mu = nn.Linear(fc2_dims, n_actions)\n",
    "            self.log_sigma = nn.Linear(fc2_dims, n_actions)\n",
    "        else:\n",
    "            # Critic outputs state value\n",
    "            self.value = nn.Linear(fc2_dims, 1)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        if self.is_actor:\n",
    "            mu = self.mu(x)\n",
    "            log_sigma = self.log_sigma(x)\n",
    "            return mu, log_sigma\n",
    "        else:\n",
    "            value = self.value(x)\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319a8dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, alpha, beta, input_dims, gamma=0.99, n_actions=1, layer1_size=64, layer2_size=64, n_outputs=1):\n",
    "        self.gamma = gamma\n",
    "        self.log_probs = None\n",
    "        self.n_outputs = n_outputs\n",
    "        \n",
    "        self.actor = GenericNetwork(alpha, input_dims, layer1_size, layer2_size, n_actions=n_actions, is_actor=True)\n",
    "        self.critic = GenericNetwork(beta, input_dims, layer1_size, layer2_size, n_actions=n_actions, is_actor=False)\n",
    "    \n",
    "    def choose_actions(self, observation, evaluate=False):\n",
    "        state = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        mu, log_sigma = self.actor(state)\n",
    "        log_sigma = T.clamp(log_sigma, -20, 2)\n",
    "        sigma = T.exp(log_sigma)\n",
    "        \n",
    "        dist = T.distributions.Normal(mu, sigma)\n",
    "        \n",
    "        if evaluate:\n",
    "            raw_action = mu\n",
    "        else:\n",
    "            raw_action = dist.rsample()\n",
    "        \n",
    "        action = T.tanh(raw_action)\n",
    "        \n",
    "        log_prob = dist.log_prob(raw_action)\n",
    "        log_prob -= T.log(1 - action.pow(2) + 1e-6)\n",
    "        self.log_probs = log_prob.sum()\n",
    "        \n",
    "        return action.detach().cpu().numpy()\n",
    "\n",
    "    def learn(self, state, reward, new_state, done):\n",
    "        state = T.tensor(state, dtype=T.float).to(self.actor.device)\n",
    "        new_state = T.tensor(new_state, dtype=T.float).to(self.actor.device)\n",
    "        reward = T.tensor(reward, dtype=T.float).to(self.actor.device)\n",
    "        \n",
    "        critic_value = self.critic(state)\n",
    "        critic_value_ = self.critic(new_state)\n",
    "        \n",
    "        delta = reward + self.gamma * critic_value_ * (1 - float(done)) - critic_value\n",
    "        \n",
    "        actor_loss = -self.log_probs * delta.detach()\n",
    "        critic_loss = delta.pow(2)\n",
    "        \n",
    "        # Separate backward passes\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "        \n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "    \n",
    "    def save_models(self, path=\"models\"):\n",
    "        T.save(self.actor.state_dict(), f\"{path}/actor.pth\")\n",
    "        T.save(self.critic.state_dict(), f\"{path}/critic.pth\")\n",
    "    \n",
    "    def load_models(self, path=\"models\"):\n",
    "        self.actor.load_state_dict(T.load(f\"{path}/actor.pth\"))\n",
    "        self.critic.load_state_dict(T.load(f\"{path}/critic.pth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdaac0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Step 100, Score: -1.37\n",
      "Episode 0, Step 200, Score: -1.80\n",
      "Episode 0, Step 300, Score: -1.95\n",
      "Episode 0, Step 400, Score: -2.05\n",
      "Episode 0, Step 500, Score: -2.12\n",
      "Episode 0, Step 600, Score: -2.20\n",
      "Episode 0, Step 700, Score: -2.25\n",
      "Episode 0, Step 800, Score: -2.28\n",
      "Episode 0, Step 900, Score: -2.31\n",
      "Episode 0, Step 1000, Score: -2.34\n",
      "Episode 0, Step 1100, Score: -2.38\n",
      "Episode 0, Step 1200, Score: -2.40\n",
      "Episode 0, Step 1300, Score: -2.42\n",
      "Episode 0, Step 1400, Score: -2.44\n",
      "Episode 0, Step 1500, Score: -2.45\n",
      "Episode 0, Step 1600, Score: -2.46\n",
      "Episode 0, Step 1700, Score: -2.48\n",
      "Episode 0, Step 1800, Score: -2.50\n",
      "Episode 0, Step 1900, Score: -2.51\n",
      "Episode 0, Step 2000, Score: -2.53\n",
      "Episode 0, Step 2100, Score: -2.54\n",
      "Episode 0, Step 2200, Score: -2.55\n",
      "Episode 0, Step 2300, Score: -2.55\n",
      "Episode 0, Step 2400, Score: -2.57\n",
      "Episode 0, Step 2500, Score: -2.58\n",
      "Episode 0, Step 2600, Score: -2.60\n",
      "Episode 0, Step 2700, Score: -2.63\n",
      "Episode 0, Step 2800, Score: -2.65\n",
      "Episode 0, Step 2900, Score: -2.69\n",
      "Episode 0, Step 3000, Score: -2.72\n",
      "Episode 0, Step 3100, Score: -2.76\n",
      "Episode 0, Step 3200, Score: -2.79\n",
      "Episode 0, Step 3300, Score: -2.84\n",
      "Episode 0, Step 3400, Score: -2.87\n",
      "Episode 0, Step 3500, Score: -3.19\n",
      "Episode 0, Step 3600, Score: -3.23\n",
      "Episode 0, Step 3700, Score: -3.24\n",
      "Episode 0, Step 3800, Score: -3.25\n",
      "Episode 0, Step 3900, Score: -3.25\n",
      "Episode 0, Step 4000, Score: -3.25\n",
      "Episode 0, Step 4100, Score: -3.26\n",
      "Episode 0, Step 4200, Score: -3.26\n",
      "Episode 0, Step 4300, Score: -3.26\n",
      "Episode 0, Step 4400, Score: -3.26\n",
      "Episode 0, Step 4500, Score: -3.26\n",
      "Episode 0, Step 4600, Score: -3.26\n",
      "Episode 0, Step 4700, Score: -3.27\n",
      "Episode 0, Step 4800, Score: -3.27\n",
      "Episode 0, Step 4900, Score: -3.27\n",
      "Episode 0, Step 5000, Score: -3.27\n",
      "Episode 0, Step 5100, Score: -3.27\n",
      "Episode 0, Step 5200, Score: -3.28\n",
      "Episode 0, Step 5300, Score: -3.28\n",
      "Episode 0, Step 5400, Score: -3.28\n",
      "Episode 0, Step 5500, Score: -3.28\n",
      "Episode 0, Step 5600, Score: -3.28\n",
      "Episode 0, Step 5700, Score: -3.29\n",
      "Episode 0, Step 5800, Score: -3.29\n",
      "Episode 0, Step 5900, Score: -3.29\n",
      "Episode 0, Step 6000, Score: -3.29\n",
      "Episode 0, Step 6100, Score: -3.29\n",
      "Episode 0, Step 6200, Score: -3.30\n",
      "Episode 0, Step 6300, Score: -3.30\n",
      "Episode 0, Step 6400, Score: -3.30\n",
      "Episode 0, Step 6500, Score: -3.30\n",
      "Episode 0, Step 6600, Score: -3.30\n",
      "Episode 0, Step 6700, Score: -3.31\n",
      "Episode 0, Step 6800, Score: -3.31\n",
      "Episode 0, Step 6900, Score: -3.31\n",
      "Episode 0, Step 7000, Score: -3.31\n",
      "Episode 0, Step 7100, Score: -3.32\n",
      "Episode 0, Step 7200, Score: -3.32\n",
      "Episode 0, Step 7300, Score: -3.32\n",
      "Episode 0, Step 7400, Score: -3.32\n",
      "Episode 0, Step 7500, Score: -3.32\n",
      "Episode 0, Step 7600, Score: -3.33\n",
      "Episode 0, Step 7700, Score: -3.33\n",
      "Episode 0, Step 7800, Score: -3.33\n",
      "Episode 0, Step 7900, Score: -3.33\n",
      "Episode 0, Step 8000, Score: -3.33\n",
      "Episode 0, Step 8100, Score: -3.34\n",
      "Episode 0, Step 8200, Score: -3.34\n",
      "Episode 0, Step 8300, Score: -3.34\n",
      "Episode 0, Step 8400, Score: -3.34\n",
      "Episode 0, Step 8500, Score: -3.34\n",
      "Episode 0, Step 8600, Score: -3.35\n",
      "Episode 0, Step 8700, Score: -3.35\n",
      "Episode 0, Step 8800, Score: -3.35\n",
      "Episode 0, Step 8900, Score: -3.35\n",
      "Episode 0, Step 9000, Score: -3.35\n",
      "Episode 0, Step 9100, Score: -3.35\n",
      "Episode 0, Step 9200, Score: -3.36\n",
      "Episode 0, Step 9300, Score: -3.36\n",
      "Episode 0, Step 9400, Score: -3.36\n",
      "Episode 0, Step 9500, Score: -3.36\n",
      "Episode 0, Step 9600, Score: -3.36\n",
      "Episode 0, Step 9700, Score: -3.36\n",
      "Episode 0, Step 9800, Score: -3.37\n",
      "Episode 0, Step 9900, Score: -3.37\n",
      "Episode 0, Step 10000, Score: -3.37\n",
      "Episode 0, Step 10100, Score: -3.37\n",
      "Episode 0, Step 10200, Score: -3.37\n",
      "Episode 0, Step 10300, Score: -3.37\n",
      "Episode 0, Step 10400, Score: -3.37\n",
      "Episode 0, Step 10500, Score: -3.37\n",
      "Episode 0, Step 10600, Score: -3.37\n",
      "Episode 0, Step 10700, Score: -3.38\n",
      "Episode 0, Step 10800, Score: -3.38\n",
      "Episode 0, Step 10900, Score: -3.38\n",
      "Episode 0, Step 11000, Score: -3.38\n",
      "Episode 0, Step 11100, Score: -3.38\n",
      "Episode 0, Step 11200, Score: -3.38\n",
      "Episode 0, Step 11300, Score: -3.38\n",
      "Episode 0, Step 11400, Score: -3.38\n",
      "Episode 0, Step 11500, Score: -3.38\n",
      "Episode 0, Step 11600, Score: -3.38\n",
      "Episode 0, Step 11700, Score: -3.38\n",
      "Episode 0, Step 11800, Score: -3.38\n",
      "Episode 0, Step 11900, Score: -3.38\n",
      "Episode 0, Step 12000, Score: -3.38\n",
      "Episode 0, Step 12100, Score: -3.38\n",
      "Episode 0, Step 12200, Score: -3.39\n",
      "Episode 0, Step 12300, Score: -3.39\n",
      "Episode 0, Step 12400, Score: -3.39\n",
      "Episode 0, Step 12500, Score: -3.39\n",
      "Episode 0, Step 12600, Score: -3.39\n",
      "Episode 0, Step 12700, Score: -3.39\n",
      "Episode 0, Step 12800, Score: -3.39\n",
      "Episode 0, Step 12900, Score: -3.39\n",
      "Episode 0, Step 13000, Score: -3.39\n",
      "Episode 0, Step 13100, Score: -3.39\n",
      "Episode 0, Step 13200, Score: -3.39\n",
      "Episode 0, Step 13300, Score: -3.39\n",
      "Episode 0, Step 13400, Score: -3.39\n",
      "Episode 0, Step 13500, Score: -3.39\n",
      "Episode 0, Step 13600, Score: -3.39\n",
      "Episode 0, Step 13700, Score: -3.39\n",
      "Episode 0, Step 13800, Score: -3.39\n",
      "Episode 0, Step 13900, Score: -3.39\n",
      "Episode 0, Step 14000, Score: -3.39\n",
      "Episode 0, Step 14100, Score: -3.39\n",
      "Episode 0, Step 14200, Score: -3.39\n",
      "Episode 0, Step 14300, Score: -3.39\n",
      "Episode 0, Step 14400, Score: -3.39\n",
      "Episode 0, Step 14500, Score: -3.39\n",
      "Episode 0, Step 14600, Score: -3.40\n",
      "Episode 0, Step 14700, Score: -3.40\n",
      "Episode 0, Step 14800, Score: -3.40\n",
      "Episode 0, Step 14900, Score: -3.40\n",
      "Episode 0, Step 15000, Score: -3.40\n",
      "Episode 0, Step 15100, Score: -3.40\n",
      "Episode 0, Step 15200, Score: -3.40\n",
      "Episode 0, Step 15300, Score: -3.40\n",
      "Episode 0, Step 15400, Score: -3.40\n",
      "Episode 0, Step 15500, Score: -3.40\n",
      "Episode 0, Step 15600, Score: -3.40\n",
      "Episode 0, Step 15700, Score: -3.40\n",
      "Episode 0, Step 15800, Score: -3.40\n",
      "Episode 0, Step 15900, Score: -3.40\n",
      "Episode 0, Step 16000, Score: -3.40\n",
      "Episode 0, Step 16100, Score: -3.40\n",
      "Episode 0, Step 16200, Score: -3.40\n",
      "Episode 0, Step 16300, Score: -3.40\n",
      "Episode 0, Step 16400, Score: -3.40\n",
      "Episode 0, Step 16500, Score: -3.40\n",
      "Episode 0, Step 16600, Score: -3.40\n",
      "Episode 0, Step 16700, Score: -3.40\n",
      "Episode 0, Step 16800, Score: -3.40\n",
      "Episode 0, Step 16900, Score: -3.40\n",
      "Episode 0, Step 17000, Score: -3.40\n",
      "Episode 0, Step 17100, Score: -3.40\n",
      "Episode 0, Step 17200, Score: -3.40\n",
      "Episode 0, Step 17300, Score: -3.96\n",
      "Episode 0, Step 17400, Score: -4.12\n",
      "Episode 0, Step 17500, Score: -4.24\n",
      "Episode 0, Step 17600, Score: -4.36\n",
      "Episode 0, Step 17700, Score: -4.41\n",
      "Episode 0, Step 17800, Score: -4.41\n",
      "Episode 0, Step 17900, Score: -4.42\n",
      "Episode 0, Step 18000, Score: -4.42\n",
      "Episode 0, Step 18100, Score: -4.43\n",
      "Episode 0, Step 18200, Score: -4.47\n",
      "Episode 0, Step 18300, Score: -4.68\n",
      "Episode 0, Step 18400, Score: -4.82\n",
      "Episode 0, Step 18500, Score: -5.35\n",
      "Episode 0, Step 18600, Score: -5.78\n",
      "Episode 0, Step 18700, Score: -6.11\n",
      "Episode 0, Step 18800, Score: -6.24\n",
      "Episode 0, Step 18900, Score: -6.25\n",
      "Episode 0, Step 19000, Score: -6.27\n",
      "Episode 0, Step 19100, Score: -6.68\n",
      "Episode 0, Step 19200, Score: -7.03\n",
      "Episode 0, Step 19300, Score: -7.11\n",
      "Episode 0, Step 19400, Score: -7.12\n",
      "Episode 0, Step 19500, Score: -7.15\n",
      "Episode 0, Step 19600, Score: -7.30\n",
      "Episode 0, Step 19700, Score: -7.66\n",
      "Episode 0, Step 19800, Score: -8.12\n",
      "Episode 0, Step 19900, Score: -8.64\n",
      "Episode 0, Step 20000, Score: -9.15\n",
      "Episode 0, Step 20100, Score: -9.72\n",
      "Episode 0, Step 20200, Score: -10.17\n",
      "Episode 0, Step 20300, Score: -10.70\n",
      "Episode  0  score: 89.25\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "agent = Agent(\n",
    "    alpha=1e-3,   \n",
    "    beta=1e-2,     \n",
    "    input_dims=env.observation_space.shape[0],\n",
    "    gamma=0.99,\n",
    "    layer1_size=256,\n",
    "    layer2_size=256\n",
    ")\n",
    "\n",
    "score_history = []\n",
    "num_episodes = 1\n",
    "for i in range(num_episodes):\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action = np.array(agent.choose_actions(observation)).reshape((1,))\n",
    "        observation_, reward, done, _, _ = env.step(action)\n",
    "        agent.learn(observation, reward, observation_, done)\n",
    "        observation = observation_\n",
    "        score += reward\n",
    "        step += 1\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Episode {i}, Step {step}, Score: {score:.2f}\")\n",
    "    \n",
    "    score_history.append(score)\n",
    "    print('Episode ', i, ' score: %.2f' % score)\n",
    "\n",
    "agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "514093d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (100,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 13\u001b[0m \u001b[43mplot_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_history\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m, in \u001b[0;36mplot_scores\u001b[0;34m(scores, window)\u001b[0m\n\u001b[1;32m      2\u001b[0m running_avg \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconvolve(\n\u001b[1;32m      3\u001b[0m     scores, np\u001b[38;5;241m.\u001b[39mones(window)\u001b[38;5;241m/\u001b[39mwindow, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(scores, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRunning Avg (100)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_rl/lib/python3.10/site-packages/matplotlib/pyplot.py:3838\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3830\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3831\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3832\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3837\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3839\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3842\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3843\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3844\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_rl/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1777\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1777\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_rl/lib/python3.10/site-packages/matplotlib/axes/_base.py:297\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 297\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_kwargs\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_rl/lib/python3.10/site-packages/matplotlib/axes/_base.py:494\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    491\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    495\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (100,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_scores(scores, window=100):\n",
    "    running_avg = np.convolve(\n",
    "        scores, np.ones(window)/window, mode='valid'\n",
    "    )\n",
    "    \n",
    "    plt.plot(scores, label=\"Score\")\n",
    "    plt.plot(range(window-1, len(scores)), running_avg, label=\"Running Avg (100)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_scores(score_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c635668",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load_models()\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0', render_mode=\"human\")\n",
    "\n",
    "for episode in range(5):\n",
    "    done = False\n",
    "    score = 0\n",
    "    observation, _ = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_actions(observation, evaluate=True)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        score += reward\n",
    "        done =  terminated or truncated\n",
    "\n",
    "    print(f\"Test Episode {episode}, Score: {score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
